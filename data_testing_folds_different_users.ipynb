{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : realworld\n",
      "Number of Classes: 8\n",
      "All Classes:  \t[0 1 2 3 4 5 6 7]\n",
      "\n",
      "Inital Step Classes: [0 1 2 3 4 5]\n",
      "Incremental Step New Classes: [6 7]\n",
      "Subjects:  [np.str_('imu\\\\1'), np.str_('imu\\\\10'), np.str_('imu\\\\11'), np.str_('imu\\\\12'), np.str_('imu\\\\13'), np.str_('imu\\\\14'), np.str_('imu\\\\15'), np.str_('imu\\\\2'), np.str_('imu\\\\3'), np.str_('imu\\\\4'), np.str_('imu\\\\5'), np.str_('imu\\\\6'), np.str_('imu\\\\7'), np.str_('imu\\\\8'), np.str_('imu\\\\9')]\n",
      "Subjects: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = 'realworld'\n",
    "\n",
    "standardize = False\n",
    "if(standardize): \n",
    "        name_add = \"_Standardized\"\n",
    "else: \n",
    "        name_add =\"\"\n",
    "if(dataset == 'mhealth'):\n",
    "    # data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\X_standarized.npy\")\n",
    "    # data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\Y_standarized.npy\")\n",
    "    # data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\pid_standarized.npy\")\n",
    "    \n",
    "    \n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    # scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1', '1618', '1637', '1639', '1642']]\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "\n",
    "if(dataset == 'Wisdm'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1616', '1618', '1637', '1639', '1642']]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:14]\n",
    "    incremental_step_classes = np.unique(data_y)[14:]\n",
    "if(dataset == 'realworld'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "\n",
    "     \n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "    # initial_step_classes = ['climbingup','climbingdown',\n",
    "    #                         'jumping','running','walking']\n",
    "    # incremental_step_classes = ['lying', 'sitting', 'standing']\n",
    "\n",
    "os.makedirs('HAR_data/'+dataset, exist_ok=True)\n",
    "\n",
    "# if(dataset == 'oppo'):\n",
    "#     data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\X.npy\")\n",
    "#     data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\Y.npy\")\n",
    "#     data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\pid.npy\")\n",
    "#     scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "#     incremental_step_classes = ['406520.0', '404505.0', '404508.0', '404511.0']\n",
    "#     initial_step_classes = [y for y in np.unique(data_y) if y not in  incremental_step_classes]\n",
    "    \n",
    "\n",
    "if(dataset == 'pamap'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]  # all the subjects\n",
    "    \n",
    "    incremental_step_classes = [10 ,18 ,19, 20, 11, 9] # Classes in Incremental Steps\n",
    "    initial_step_classes = np.array([y for y in np.unique(data_y) if y not in  incremental_step_classes])\n",
    "    \n",
    "    print(\"Classes before transforming\", np.unique(data_y))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = np.array(le.transform(data_y))\n",
    "    \n",
    "    \n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    initial_step_classes = le.transform(initial_step_classes)\n",
    "    incremental_step_classes = le.transform(incremental_step_classes)\n",
    "\n",
    "    print(\"Classes After transforming\", np.unique(data_y))\n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    classes = np.concatenate([initial_step_classes , incremental_step_classes])\n",
    "\n",
    "    print(\"Classes after ordering the transformed classes\")\n",
    "    changed_data_y = []  # Changing the order of the classes\n",
    "    for i,clas in enumerate(classes):\n",
    "        data_y[data_y == clas] = -i\n",
    "    data_y = -data_y\n",
    "    \n",
    "    initial_step_classes =np.unique(data_y)[:len(initial_step_classes)]\n",
    "    incremental_step_classes = np.unique(data_y)[len(initial_step_classes):]\n",
    "    \n",
    "\n",
    "# Scenario 1\n",
    "# Here Scenario 1 is where we use same test data for both initial and incremental step.\n",
    "\n",
    "scenario =1\n",
    "print(\"Dataset :\", dataset)\n",
    "print(\"Number of Classes: {}\\nAll Classes:  \\t{}\".format(len(np.unique(data_y)), np.unique(data_y)))\n",
    "print(\"\\nInital Step Classes: {}\\nIncremental Step New Classes: {}\".format(initial_step_classes, incremental_step_classes))\n",
    "print('Subjects: ',scenario1_step_subjects)\n",
    "print('Subjects:', len(scenario1_step_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# kf.split(np.unique(data_p))\n",
    "# for fold, (train_index, test_index) in enumerate(kf.split(np.unique(data_p))):\n",
    "#     print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from collections import Counter\n",
    "\n",
    "def reduce_frequency(arrs,target_array, target_elements, freq_val):\n",
    "    # Step 1: Get frequency of each element\n",
    "    freq = Counter(target_array)\n",
    "    \n",
    "    # Step 2: Adjust frequency for target elements\n",
    "    for elem in target_elements:\n",
    "        if elem in freq:\n",
    "            # Calculate target frequency (20% of original frequency)\n",
    "            target_freq = int(freq[elem] * float(freq_val))\n",
    "            \n",
    "            # Randomly remove instances of the element from the array\n",
    "            indices = [i for i, x in enumerate(target_array) if x == elem]\n",
    "            indices_to_remove = random.sample(indices, freq[elem] - target_freq)\n",
    "            \n",
    "            for arr in arrs:\n",
    "                for i in sorted(indices_to_remove, reverse=True):\n",
    "                    arr.pop(i)\n",
    "    \n",
    "    return arrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 0, Train Subjects: ['imu\\\\10', 'imu\\\\11', 'imu\\\\12', 'imu\\\\13', 'imu\\\\14', 'imu\\\\15', 'imu\\\\2', 'imu\\\\3', 'imu\\\\5', 'imu\\\\7', 'imu\\\\8', 'imu\\\\9'], Test Subjects: ['imu\\\\1', 'imu\\\\4', 'imu\\\\6'] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: Seen: 36342, Unseen: 53457\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 32707, Unique Classes: 6, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15'], Classes Count: Counter({np.int64(3): 6900, np.int64(5): 6814, np.int64(4): 6782, np.int64(1): 6163, np.int64(0): 4979, np.int64(2): 1069})\n",
      "Val: Len: 3635, Unique Classes: 6, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15'], Classes Count: Counter({np.int64(3): 767, np.int64(5): 757, np.int64(4): 754, np.int64(1): 685, np.int64(0): 553, np.int64(2): 119})\n",
      "Test: Len: 19854, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\4' 'imu\\\\6'], Classes Count: Counter({np.int64(4): 4606, np.int64(5): 3879, np.int64(3): 3864, np.int64(1): 3638, np.int64(0): 3349, np.int64(2): 518})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 13720, Unique Classes: 2, People: ['imu\\\\2' 'imu\\\\3' 'imu\\\\5' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'], Classes Count: Counter({np.int64(7): 6956, np.int64(6): 6764}) \n",
      "Val: Len: 5346, Unique Classes: 8, People: ['imu\\\\2' 'imu\\\\3' 'imu\\\\5' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'], Classes Count: Counter({np.int64(4): 869, np.int64(7): 773, np.int64(5): 765, np.int64(3): 760, np.int64(6): 751, np.int64(1): 703, np.int64(0): 609, np.int64(2): 116}) \n",
      "Test: Len: 27337 , Unique Classes: 8, People: ['imu\\\\1' 'imu\\\\4' 'imu\\\\6'], Classes Count: Counter({np.int64(4): 4606, np.int64(5): 3879, np.int64(3): 3864, np.int64(7): 3749, np.int64(6): 3734, np.int64(1): 3638, np.int64(0): 3349, np.int64(2): 518})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 1, Train Subjects: ['imu\\\\1', 'imu\\\\10', 'imu\\\\11', 'imu\\\\12', 'imu\\\\13', 'imu\\\\15', 'imu\\\\2', 'imu\\\\4', 'imu\\\\5', 'imu\\\\6', 'imu\\\\7', 'imu\\\\9'], Test Subjects: ['imu\\\\14', 'imu\\\\3', 'imu\\\\8'] \n",
      "\n",
      "Total Data: Seen: 36383, Unseen: 53428\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 32744, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\15'], Classes Count: Counter({np.int64(3): 6896, np.int64(5): 6866, np.int64(4): 6767, np.int64(1): 6025, np.int64(0): 5143, np.int64(2): 1047})\n",
      "Val: Len: 3639, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\13' 'imu\\\\15'], Classes Count: Counter({np.int64(3): 766, np.int64(5): 763, np.int64(4): 752, np.int64(1): 670, np.int64(0): 572, np.int64(2): 116})\n",
      "Test: Len: 19611, Unique Classes: 6, People: ['imu\\\\14' 'imu\\\\3' 'imu\\\\8'], Classes Count: Counter({np.int64(1): 4704, np.int64(4): 3960, np.int64(5): 3800, np.int64(3): 3758, np.int64(0): 2800, np.int64(2): 589})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 13512, Unique Classes: 2, People: ['imu\\\\2' 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\9'], Classes Count: Counter({np.int64(7): 6797, np.int64(6): 6715}) \n",
      "Val: Len: 5343, Unique Classes: 8, People: ['imu\\\\2' 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\9'], Classes Count: Counter({np.int64(4): 935, np.int64(3): 771, np.int64(5): 767, np.int64(7): 755, np.int64(6): 746, np.int64(0): 646, np.int64(1): 611, np.int64(2): 112}) \n",
      "Test: Len: 27353 , Unique Classes: 8, People: ['imu\\\\14' 'imu\\\\3' 'imu\\\\8'], Classes Count: Counter({np.int64(1): 4704, np.int64(7): 3989, np.int64(4): 3960, np.int64(5): 3800, np.int64(3): 3758, np.int64(6): 3753, np.int64(0): 2800, np.int64(2): 589})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 2, Train Subjects: ['imu\\\\1', 'imu\\\\12', 'imu\\\\13', 'imu\\\\14', 'imu\\\\15', 'imu\\\\2', 'imu\\\\3', 'imu\\\\4', 'imu\\\\5', 'imu\\\\6', 'imu\\\\7', 'imu\\\\8'], Test Subjects: ['imu\\\\10', 'imu\\\\11', 'imu\\\\9'] \n",
      "\n",
      "Total Data: Seen: 34621, Unseen: 56566\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 31158, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2'], Classes Count: Counter({np.int64(5): 6830, np.int64(3): 6810, np.int64(4): 6206, np.int64(1): 5187, np.int64(0): 5098, np.int64(2): 1027})\n",
      "Val: Len: 3463, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\12' 'imu\\\\13' 'imu\\\\14' 'imu\\\\15' 'imu\\\\2'], Classes Count: Counter({np.int64(5): 759, np.int64(3): 757, np.int64(4): 690, np.int64(1): 576, np.int64(0): 567, np.int64(2): 114})\n",
      "Test: Len: 18449, Unique Classes: 6, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\9'], Classes Count: Counter({np.int64(4): 4133, np.int64(3): 3825, np.int64(5): 3803, np.int64(1): 3241, np.int64(0): 2850, np.int64(2): 597})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 13704, Unique Classes: 2, People: ['imu\\\\3' 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8'], Classes Count: Counter({np.int64(7): 6971, np.int64(6): 6733}) \n",
      "Val: Len: 5657, Unique Classes: 8, People: ['imu\\\\3' 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8'], Classes Count: Counter({np.int64(4): 980, np.int64(1): 851, np.int64(7): 775, np.int64(3): 774, np.int64(5): 770, np.int64(6): 748, np.int64(0): 646, np.int64(2): 113}) \n",
      "Test: Len: 26114 , Unique Classes: 8, People: ['imu\\\\10' 'imu\\\\11' 'imu\\\\9'], Classes Count: Counter({np.int64(4): 4133, np.int64(6): 3840, np.int64(3): 3825, np.int64(7): 3825, np.int64(5): 3803, np.int64(1): 3241, np.int64(0): 2850, np.int64(2): 597})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 3, Train Subjects: ['imu\\\\1', 'imu\\\\10', 'imu\\\\11', 'imu\\\\12', 'imu\\\\14', 'imu\\\\15', 'imu\\\\3', 'imu\\\\4', 'imu\\\\6', 'imu\\\\7', 'imu\\\\8', 'imu\\\\9'], Test Subjects: ['imu\\\\13', 'imu\\\\2', 'imu\\\\5'] \n",
      "\n",
      "Total Data: Seen: 36293, Unseen: 55711\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 32663, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\14' 'imu\\\\15'], Classes Count: Counter({np.int64(3): 6864, np.int64(5): 6839, np.int64(4): 6769, np.int64(1): 6057, np.int64(0): 5105, np.int64(2): 1029})\n",
      "Val: Len: 3630, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\12' 'imu\\\\14' 'imu\\\\15'], Classes Count: Counter({np.int64(3): 763, np.int64(5): 760, np.int64(4): 753, np.int64(1): 673, np.int64(0): 567, np.int64(2): 114})\n",
      "Test: Len: 17579, Unique Classes: 6, People: ['imu\\\\13' 'imu\\\\2' 'imu\\\\5'], Classes Count: Counter({np.int64(4): 4102, np.int64(3): 3834, np.int64(5): 3805, np.int64(0): 2856, np.int64(1): 2399, np.int64(2): 583})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 13656, Unique Classes: 2, People: ['imu\\\\3' 'imu\\\\4' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'], Classes Count: Counter({np.int64(7): 6848, np.int64(6): 6808}) \n",
      "Val: Len: 5572, Unique Classes: 8, People: ['imu\\\\3' 'imu\\\\4' 'imu\\\\6' 'imu\\\\7' 'imu\\\\8' 'imu\\\\9'], Classes Count: Counter({np.int64(4): 920, np.int64(1): 839, np.int64(5): 769, np.int64(3): 767, np.int64(7): 761, np.int64(6): 757, np.int64(0): 645, np.int64(2): 114}) \n",
      "Test: Len: 25242 , Unique Classes: 8, People: ['imu\\\\13' 'imu\\\\2' 'imu\\\\5'], Classes Count: Counter({np.int64(4): 4102, np.int64(7): 3901, np.int64(3): 3834, np.int64(5): 3805, np.int64(6): 3762, np.int64(0): 2856, np.int64(1): 2399, np.int64(2): 583})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 4, Train Subjects: ['imu\\\\1', 'imu\\\\10', 'imu\\\\11', 'imu\\\\13', 'imu\\\\14', 'imu\\\\2', 'imu\\\\3', 'imu\\\\4', 'imu\\\\5', 'imu\\\\6', 'imu\\\\8', 'imu\\\\9'], Test Subjects: ['imu\\\\12', 'imu\\\\15', 'imu\\\\7'] \n",
      "\n",
      "Total Data: Seen: 34361, Unseen: 56340\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 30924, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\13' 'imu\\\\14' 'imu\\\\2'], Classes Count: Counter({np.int64(3): 6832, np.int64(5): 6825, np.int64(4): 6153, np.int64(1): 5087, np.int64(0): 4989, np.int64(2): 1038})\n",
      "Val: Len: 3437, Unique Classes: 6, People: ['imu\\\\1' 'imu\\\\10' 'imu\\\\11' 'imu\\\\13' 'imu\\\\14' 'imu\\\\2'], Classes Count: Counter({np.int64(3): 759, np.int64(5): 759, np.int64(4): 684, np.int64(1): 565, np.int64(0): 555, np.int64(2): 115})\n",
      "Test: Len: 18916, Unique Classes: 6, People: ['imu\\\\12' 'imu\\\\15' 'imu\\\\7'], Classes Count: Counter({np.int64(4): 4027, np.int64(3): 3847, np.int64(5): 3811, np.int64(1): 3532, np.int64(0): 3118, np.int64(2): 581})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 13687, Unique Classes: 2, People: ['imu\\\\3' 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\8' 'imu\\\\9'], Classes Count: Counter({np.int64(7): 6989, np.int64(6): 6698}) \n",
      "Val: Len: 5634, Unique Classes: 8, People: ['imu\\\\3' 'imu\\\\4' 'imu\\\\5' 'imu\\\\6' 'imu\\\\8' 'imu\\\\9'], Classes Count: Counter({np.int64(4): 996, np.int64(1): 833, np.int64(7): 777, np.int64(5): 770, np.int64(3): 769, np.int64(6): 744, np.int64(0): 631, np.int64(2): 114}) \n",
      "Test: Len: 26503 , Unique Classes: 8, People: ['imu\\\\12' 'imu\\\\15' 'imu\\\\7'], Classes Count: Counter({np.int64(4): 4027, np.int64(3): 3847, np.int64(5): 3811, np.int64(7): 3801, np.int64(6): 3786, np.int64(1): 3532, np.int64(0): 3118, np.int64(2): 581})\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(np.unique(data_p))):\n",
    "    data_seen_classes_scenario_1_x,data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p = [], [], []\n",
    "    data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p = [], [], []\n",
    "    data_test_scenario_1_x, data_test_scenario_1_y,data_test_scenario_1_p = [], [], []\n",
    "    data_incremental_step_x_test, data_incremental_step_y_test, data_incremental_step_p_test = [], [], []\n",
    "    data_initial_step_x_test, data_initial_step_y_test, data_initial_step_p_test = [], [], []\n",
    "    \n",
    "    train_subjects_in_fold = np.unique(data_p)[train_index].tolist()\n",
    "\n",
    "    # Half of the total subjects are in initial step and half are used in incremental step.\n",
    "    initial_subjects_in_fold = train_subjects_in_fold[0:len(train_subjects_in_fold)//2]\n",
    "    incremental_subjects_in_fold = train_subjects_in_fold[len(train_subjects_in_fold)//2:]\n",
    "\n",
    "    \n",
    "\n",
    "    test_subjects_in_fold = np.unique(data_p)[test_index].tolist()\n",
    "\n",
    "    \n",
    "    # print(type(fold), type(list(train_subjects_in_fold.tolist())[0]))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "    print(f\"\\n Fold {fold}, Train Subjects: {train_subjects_in_fold}, Test Subjects: {test_subjects_in_fold} \\n\")\n",
    "    for index,person in enumerate(data_p):\n",
    "        # print(person, train_subjects_in_fold, data_y[index], initial_step_classes)\n",
    "        \n",
    "        # Only including the intial step classes for initial step subject\n",
    "        if(person in initial_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "            \n",
    "            data_seen_classes_scenario_1_x.append(data_x[index])\n",
    "            data_seen_classes_scenario_1_y.append(data_y[index])\n",
    "            data_seen_classes_scenario_1_p.append(data_p[index])\n",
    "\n",
    "        \n",
    "\n",
    "        from collections import Counter\n",
    "        # Including all the classes : Old + New for incremental step subjects.\n",
    "        if(person in incremental_subjects_in_fold and data_y[index] in np.unique(data_y)):\n",
    "\n",
    "            data_unseen_classes_scenario_1_x.append(data_x[index])\n",
    "            data_unseen_classes_scenario_1_y.append(data_y[index])\n",
    "            data_unseen_classes_scenario_1_p.append(data_p[index])\n",
    "        \n",
    "        # For initial step test dataset we only include initial step classes : Old Classes\n",
    "        if(person in test_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "            data_initial_step_x_test.append(data_x[index])\n",
    "            data_initial_step_y_test.append(data_y[index])\n",
    "            data_initial_step_p_test.append(data_p[index])\n",
    "\n",
    "        # For Incremental step test dataset we all the classes : Old + New Classes\n",
    "        if(person in test_subjects_in_fold):\n",
    "            \n",
    "            data_incremental_step_x_test.append(data_x[index])\n",
    "            data_incremental_step_y_test.append(data_y[index])\n",
    "            data_incremental_step_p_test.append(data_p[index])\n",
    "\n",
    "        # if(((person in train_subjects_in_fold and data_y[index] in initial_step_classes) or (person in train_subjects_in_fold and data_y[index] in incremental_step_classes)\n",
    "        #     or (person in test_subjects_in_fold and data_y[index] in initial_step_classes) or (person in test_subjects_in_fold)) == False):\n",
    "        #     print(\"Some Issue\")   \n",
    "        # # else:\n",
    "           \n",
    "        # #     print(\"Some thing is left..\") # Just to check if any element falls outside our conditions.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Doing Split of Initial Step for Validation Data\n",
    "    data_initial_step_scenario_1_x_train, data_initial_step_scenario_1_x_val, data_initial_step_scenario_1_y_train, data_initial_step_scenario_1_y_val, data_initial_step_scenario_1_p_train, data_initial_step_scenario_1_p_val = train_test_split(data_seen_classes_scenario_1_x, data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p, test_size=0.10, stratify=(data_seen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "\n",
    "    # UnSeen Classes Train and validation Split for Incremental Step. \n",
    "    # Reduce the frequency of seen classes in the incremetnal step\n",
    "    # data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p =  reduce_frequency([data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p], data_unseen_classes_scenario_1_y, initial_step_classes , 1)\n",
    "   \n",
    "    # Getting Validation Data\n",
    "    data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_x_val, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_y_val, data_incremental_step_scenario_1_p_train, data_incremental_step_scenario_1_p_val = train_test_split(data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p, test_size=0.1, stratify=(data_unseen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "    # Removing some of the seen data from the training step.\n",
    "    data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_p_train = reduce_frequency([data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_p_train], data_incremental_step_scenario_1_y_train, initial_step_classes , 0.0)\n",
    "    \n",
    "    print(f\"Total Data: Seen: {len(data_seen_classes_scenario_1_x)}, Unseen: {len(data_unseen_classes_scenario_1_x)}\")\n",
    "    print(\"\\n ********* Inital ************** \\n\")\n",
    "    print(f\"Train: Len: {len(data_initial_step_scenario_1_x_train)}, Unique Classes: {len(np.unique(data_initial_step_scenario_1_y_train))}, People: {np.unique(data_initial_step_scenario_1_p_train)}, Classes Count: {Counter(data_initial_step_scenario_1_y_train)}\")\n",
    "    print(f\"Val: Len: {len(data_initial_step_scenario_1_x_val)}, Unique Classes: {len(np.unique(data_initial_step_scenario_1_y_val))}, People: {np.unique(data_initial_step_scenario_1_p_val)}, Classes Count: {Counter(data_initial_step_scenario_1_y_val)}\") \n",
    "    print(f\"Test: Len: {len(data_initial_step_x_test)}, Unique Classes: {len(np.unique(data_initial_step_y_test))}, People: {np.unique(data_initial_step_p_test)}, Classes Count: {Counter(data_initial_step_y_test)}\")\n",
    "\n",
    "    print(\"\\n ********* Incremantal ************** \\n\")\n",
    "    print(f\"Train: Len: {len(data_incremental_step_scenario_1_x_train)}, Unique Classes: {len(np.unique(data_incremental_step_scenario_1_y_train))}, People: {np.unique(data_incremental_step_scenario_1_p_train)}, Classes Count: {Counter(data_incremental_step_scenario_1_y_train)} \")\n",
    "    print(f\"Val: Len: {len(data_incremental_step_scenario_1_x_val)}, Unique Classes: {len(np.unique(data_incremental_step_scenario_1_y_val))}, People: {np.unique(data_incremental_step_scenario_1_p_val)}, Classes Count: {Counter(data_incremental_step_scenario_1_y_val)} \")    \n",
    "    print(f\"Test: Len: {len(data_incremental_step_x_test)} , Unique Classes: {len(np.unique(data_incremental_step_y_test))}, People: {np.unique(data_incremental_step_p_test)}, Classes Count: {Counter(data_incremental_step_y_test)}\")\n",
    "    \n",
    "\n",
    "    # Saving the necessary files\n",
    "    windowlen = np.array(data_incremental_step_scenario_1_x_train).shape[1]\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_p_train)\n",
    "\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_initial_step_p_test)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_p_train)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}.npy',data_incremental_step_p_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({np.int64(4): 9964, np.int64(1): 8330, np.int64(7): 7766, np.int64(5): 7703, np.int64(3): 7690, np.int64(6): 7442, np.int64(0): 6311, np.int64(2): 1134})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(data_unseen_classes_scenario_1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({np.int64(4): 9964, np.int64(1): 8330, np.int64(7): 7766, np.int64(5): 7703, np.int64(3): 7690, np.int64(6): 7442, np.int64(0): 6311, np.int64(2): 1134})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(data_unseen_classes_scenario_1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climbingdown' 'climbingup' 'jumping' 'lying' 'running' 'sitting'\n",
      " 'standing' 'walking']\n",
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "\n",
    "print(le.fit_transform(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"personid\":data_p, \"activityid\":data_y})\n",
    "activities_by_person = df.groupby('personid')['activityid'].nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personid</th>\n",
       "      <th>activityid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imu\\1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imu\\10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imu\\11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imu\\12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imu\\13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>imu\\14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>imu\\15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>imu\\2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>imu\\3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>imu\\4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>imu\\5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>imu\\6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>imu\\7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>imu\\8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>imu\\9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personid  activityid\n",
       "0     imu\\1           8\n",
       "1    imu\\10           8\n",
       "2    imu\\11           8\n",
       "3    imu\\12           8\n",
       "4    imu\\13           8\n",
       "5    imu\\14           8\n",
       "6    imu\\15           8\n",
       "7     imu\\2           7\n",
       "8     imu\\3           8\n",
       "9     imu\\4           8\n",
       "10    imu\\5           8\n",
       "11    imu\\6           8\n",
       "12    imu\\7           8\n",
       "13    imu\\8           8\n",
       "14    imu\\9           8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_by_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
