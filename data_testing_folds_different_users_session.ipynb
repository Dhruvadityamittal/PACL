{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : mhealth\n",
      "Number of Classes: 12\n",
      "All Classes:  \t[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "\n",
      "Inital Step Classes: [0 1 2 3 4 5]\n",
      "Incremental Step New Classes: [ 6  7  8  9 10 11]\n",
      "Subjects:  [np.str_('1'), np.str_('10'), np.str_('2'), np.str_('3'), np.str_('4'), np.str_('5'), np.str_('6'), np.str_('7'), np.str_('8'), np.str_('9')]\n",
      "Subjects: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = 'mhealth'\n",
    "\n",
    "standardize = False\n",
    "if(standardize): \n",
    "        name_add = \"_Standardized\"\n",
    "else: \n",
    "        name_add =\"\"\n",
    "if(dataset == 'mhealth'):\n",
    "    # data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\X_standarized.npy\")\n",
    "    # data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\Y_standarized.npy\")\n",
    "    # data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\mhealth+dataset\\data\\pid_standarized.npy\")\n",
    "    \n",
    "    \n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\mhealth+dataset\\data\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    # scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1', '1618', '1637', '1639', '1642']]\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "\n",
    "if(dataset == 'Wisdm'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    # data_time = np.load(r\"C:\\Users\\Dhruv\\Downloads\\Compressed\\wisdm+smartphone+and+smartwatch+activity+and+biometrics+dataset\\wisdm-dataset\\wisdm-dataset\\wisdm_30hz_w10\\time.npy\")\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p) if str(personid) not in ['1616', '1618', '1637', '1639', '1642']]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "    \n",
    "    initial_step_classes = np.unique(data_y)[0:14]\n",
    "    incremental_step_classes = np.unique(data_y)[14:]\n",
    "if(dataset == 'realworld'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\RealWorld\\realworld_30hz_w10\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = le.transform(data_y)\n",
    "\n",
    "     \n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "    initial_step_classes = np.unique(data_y)[0:6]\n",
    "    incremental_step_classes = np.unique(data_y)[6:]\n",
    "    # initial_step_classes = ['climbingup','climbingdown',\n",
    "    #                         'jumping','running','walking']\n",
    "    # incremental_step_classes = ['lying', 'sitting', 'standing']\n",
    "\n",
    "os.makedirs('HAR_data/'+dataset, exist_ok=True)\n",
    "\n",
    "# if(dataset == 'oppo'):\n",
    "#     data_x = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\X.npy\")\n",
    "#     data_y= np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\Y.npy\")\n",
    "#     data_p = np.load(r\"C:\\Users\\Dhruv\\Downloads\\opportunity+activity+recognition\\OpportunityUCIDataset\\data\\pid.npy\")\n",
    "#     scenario1_step_subjects = [personid for personid in np.unique(data_p)]\n",
    "#     incremental_step_classes = ['406520.0', '404505.0', '404508.0', '404511.0']\n",
    "#     initial_step_classes = [y for y in np.unique(data_y) if y not in  incremental_step_classes]\n",
    "    \n",
    "\n",
    "if(dataset == 'pamap'):\n",
    "    data_x = np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\X{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_y= np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\Y{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    data_p = np.load(fr\"{os.getcwd()}\\Datasets\\pamap2+physical+activity+monitoring\\PAMAP2_Dataset\\PAMAP2_Dataset\\pid{name_add}.npy\".replace(\"\\\\\",\"/\"))\n",
    "    scenario1_step_subjects = [personid for personid in np.unique(data_p)]  # all the subjects\n",
    "    \n",
    "    incremental_step_classes = [10 ,18 ,19, 20, 11, 9] # Classes in Incremental Steps\n",
    "    initial_step_classes = np.array([y for y in np.unique(data_y) if y not in  incremental_step_classes])\n",
    "    \n",
    "    print(\"Classes before transforming\", np.unique(data_y))\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.unique(data_y))\n",
    "    data_y = np.array(le.transform(data_y))\n",
    "    \n",
    "    \n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    initial_step_classes = le.transform(initial_step_classes)\n",
    "    incremental_step_classes = le.transform(incremental_step_classes)\n",
    "\n",
    "    print(\"Classes After transforming\", np.unique(data_y))\n",
    "    print(initial_step_classes,incremental_step_classes )\n",
    "    classes = np.concatenate([initial_step_classes , incremental_step_classes])\n",
    "\n",
    "    print(\"Classes after ordering the transformed classes\")\n",
    "    changed_data_y = []  # Changing the order of the classes\n",
    "    for i,clas in enumerate(classes):\n",
    "        data_y[data_y == clas] = -i\n",
    "    data_y = -data_y\n",
    "    \n",
    "    initial_step_classes =np.unique(data_y)[:len(initial_step_classes)]\n",
    "    incremental_step_classes = np.unique(data_y)[len(initial_step_classes):]\n",
    "    \n",
    "\n",
    "# Scenario 1\n",
    "# Here Scenario 1 is where we use same test data for both initial and incremental step.\n",
    "\n",
    "scenario =1\n",
    "print(\"Dataset :\", dataset)\n",
    "print(\"Number of Classes: {}\\nAll Classes:  \\t{}\".format(len(np.unique(data_y)), np.unique(data_y)))\n",
    "print(\"\\nInital Step Classes: {}\\nIncremental Step New Classes: {}\".format(initial_step_classes, incremental_step_classes))\n",
    "print('Subjects: ',scenario1_step_subjects)\n",
    "print('Subjects:', len(scenario1_step_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from collections import Counter\n",
    "\n",
    "def reduce_frequency(arrs,target_array, target_elements, freq_val):\n",
    "    # Step 1: Get frequency of each element\n",
    "    freq = Counter(target_array)\n",
    "    \n",
    "    # Step 2: Adjust frequency for target elements\n",
    "    for elem in target_elements:\n",
    "        if elem in freq:\n",
    "            # Calculate target frequency (20% of original frequency)\n",
    "            target_freq = int(freq[elem] * float(freq_val))\n",
    "            \n",
    "            # Randomly remove instances of the element from the array\n",
    "            indices = [i for i, x in enumerate(target_array) if x == elem]\n",
    "            indices_to_remove = random.sample(indices, freq[elem] - target_freq)\n",
    "            \n",
    "            for arr in arrs:\n",
    "                for i in sorted(indices_to_remove, reverse=True):\n",
    "                    arr.pop(i)\n",
    "    \n",
    "    return arrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 0, Initial Subjects: ['8' '10' '5' '1' '7'], Incremental Subjects: ['2' '9' '4' '3' '6'] \n",
      "\n",
      "Total Data: Seen: 2770, Unseen: 5324\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 2493, Unique Classes: 6, People: ['1' '10' '5' '7' '8'], Classes Count: Counter({np.int64(1): 428, np.int64(3): 428, np.int64(2): 427, np.int64(0): 427, np.int64(4): 411, np.int64(5): 372})\n",
      "Val: Len: 277, Unique Classes: 6, People: ['1' '10' '5' '7' '8'], Classes Count: Counter({np.int64(0): 48, np.int64(2): 48, np.int64(3): 47, np.int64(1): 47, np.int64(4): 46, np.int64(5): 41})\n",
      "Test: Len: 703, Unique Classes: 6, People: ['1' '10' '5' '7' '8'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 117, np.int64(5): 106})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 4791, Unique Classes: 12, People: ['2' '3' '4' '6' '9'], Classes Count: Counter({np.int64(1): 428, np.int64(2): 428, np.int64(3): 427, np.int64(10): 427, np.int64(9): 427, np.int64(0): 427, np.int64(4): 427, np.int64(8): 427, np.int64(7): 416, np.int64(6): 415, np.int64(5): 410, np.int64(11): 132}) \n",
      "Val: Len: 533, Unique Classes: 12, People: ['2' '3' '4' '6' '9'], Classes Count: Counter({np.int64(9): 48, np.int64(10): 48, np.int64(4): 48, np.int64(8): 48, np.int64(3): 47, np.int64(2): 47, np.int64(0): 47, np.int64(1): 47, np.int64(6): 46, np.int64(5): 46, np.int64(7): 46, np.int64(11): 15}) \n",
      "Test: Len: 1351 , Unique Classes: 12, People: ['2' '3' '4' '6' '9'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 120, np.int64(8): 120, np.int64(9): 120, np.int64(10): 120, np.int64(6): 118, np.int64(7): 118, np.int64(5): 115, np.int64(11): 40})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 1, Initial Subjects: ['1' '10' '8' '5' '3'], Incremental Subjects: ['4' '7' '9' '6' '2'] \n",
      "\n",
      "Total Data: Seen: 2789, Unseen: 5273\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 2510, Unique Classes: 6, People: ['1' '10' '3' '5' '8'], Classes Count: Counter({np.int64(0): 428, np.int64(2): 427, np.int64(3): 427, np.int64(1): 427, np.int64(4): 425, np.int64(5): 376})\n",
      "Val: Len: 279, Unique Classes: 6, People: ['1' '10' '3' '5' '8'], Classes Count: Counter({np.int64(2): 48, np.int64(1): 48, np.int64(4): 47, np.int64(0): 47, np.int64(3): 47, np.int64(5): 42})\n",
      "Test: Len: 707, Unique Classes: 6, People: ['1' '10' '3' '5' '8'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 120, np.int64(5): 107})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 4745, Unique Classes: 12, People: ['2' '4' '6' '7' '9'], Classes Count: Counter({np.int64(2): 428, np.int64(1): 428, np.int64(9): 427, np.int64(0): 427, np.int64(8): 427, np.int64(10): 427, np.int64(3): 427, np.int64(4): 414, np.int64(7): 406, np.int64(5): 406, np.int64(6): 396, np.int64(11): 132}) \n",
      "Val: Len: 528, Unique Classes: 12, People: ['2' '4' '6' '7' '9'], Classes Count: Counter({np.int64(8): 48, np.int64(10): 48, np.int64(3): 48, np.int64(9): 48, np.int64(1): 47, np.int64(0): 47, np.int64(2): 47, np.int64(4): 46, np.int64(7): 45, np.int64(5): 45, np.int64(6): 44, np.int64(11): 15}) \n",
      "Test: Len: 1339 , Unique Classes: 12, People: ['2' '4' '6' '7' '9'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(8): 120, np.int64(9): 120, np.int64(10): 120, np.int64(4): 117, np.int64(7): 115, np.int64(5): 114, np.int64(6): 113, np.int64(11): 40})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 2, Initial Subjects: ['9' '2' '1' '6' '8'], Incremental Subjects: ['5' '3' '7' '10' '4'] \n",
      "\n",
      "Total Data: Seen: 2783, Unseen: 5299\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 2504, Unique Classes: 6, People: ['1' '2' '6' '8' '9'], Classes Count: Counter({np.int64(2): 427, np.int64(3): 427, np.int64(0): 427, np.int64(1): 427, np.int64(4): 425, np.int64(5): 371})\n",
      "Val: Len: 279, Unique Classes: 6, People: ['1' '2' '6' '8' '9'], Classes Count: Counter({np.int64(2): 48, np.int64(3): 48, np.int64(1): 48, np.int64(4): 47, np.int64(0): 47, np.int64(5): 41})\n",
      "Test: Len: 705, Unique Classes: 6, People: ['1' '2' '6' '8' '9'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 120, np.int64(5): 105})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 4769, Unique Classes: 12, People: ['10' '3' '4' '5' '7'], Classes Count: Counter({np.int64(0): 427, np.int64(3): 427, np.int64(2): 427, np.int64(10): 427, np.int64(1): 427, np.int64(8): 427, np.int64(9): 427, np.int64(6): 416, np.int64(4): 414, np.int64(5): 411, np.int64(7): 408, np.int64(11): 131}) \n",
      "Val: Len: 530, Unique Classes: 12, People: ['10' '3' '4' '5' '7'], Classes Count: Counter({np.int64(10): 48, np.int64(0): 48, np.int64(2): 48, np.int64(1): 48, np.int64(9): 47, np.int64(8): 47, np.int64(3): 47, np.int64(6): 46, np.int64(4): 46, np.int64(5): 46, np.int64(7): 45, np.int64(11): 14}) \n",
      "Test: Len: 1349 , Unique Classes: 12, People: ['10' '3' '4' '5' '7'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(6): 120, np.int64(8): 120, np.int64(9): 120, np.int64(10): 120, np.int64(4): 117, np.int64(5): 116, np.int64(7): 116, np.int64(11): 40})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 3, Initial Subjects: ['10' '7' '6' '2' '8'], Incremental Subjects: ['1' '3' '4' '5' '9'] \n",
      "\n",
      "Total Data: Seen: 2756, Unseen: 5367\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 2480, Unique Classes: 6, People: ['10' '2' '6' '7' '8'], Classes Count: Counter({np.int64(1): 428, np.int64(2): 427, np.int64(3): 427, np.int64(0): 427, np.int64(4): 411, np.int64(5): 360})\n",
      "Val: Len: 276, Unique Classes: 6, People: ['10' '2' '6' '7' '8'], Classes Count: Counter({np.int64(2): 48, np.int64(3): 48, np.int64(1): 47, np.int64(0): 47, np.int64(4): 46, np.int64(5): 40})\n",
      "Test: Len: 699, Unique Classes: 6, People: ['10' '2' '6' '7' '8'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 117, np.int64(5): 102})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 4830, Unique Classes: 12, People: ['1' '3' '4' '5' '9'], Classes Count: Counter({np.int64(6): 429, np.int64(1): 428, np.int64(10): 428, np.int64(0): 427, np.int64(3): 427, np.int64(8): 427, np.int64(7): 427, np.int64(9): 427, np.int64(4): 427, np.int64(2): 427, np.int64(5): 422, np.int64(11): 134}) \n",
      "Val: Len: 537, Unique Classes: 12, People: ['1' '3' '4' '5' '9'], Classes Count: Counter({np.int64(8): 48, np.int64(2): 48, np.int64(6): 48, np.int64(0): 48, np.int64(4): 48, np.int64(5): 47, np.int64(7): 47, np.int64(9): 47, np.int64(10): 47, np.int64(3): 47, np.int64(1): 47, np.int64(11): 15}) \n",
      "Test: Len: 1363 , Unique Classes: 12, People: ['1' '3' '4' '5' '9'], Classes Count: Counter({np.int64(6): 123, np.int64(7): 121, np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 120, np.int64(8): 120, np.int64(9): 120, np.int64(10): 120, np.int64(5): 119, np.int64(11): 40})\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Fold 4, Initial Subjects: ['10' '5' '4' '8' '1'], Incremental Subjects: ['7' '6' '3' '2' '9'] \n",
      "\n",
      "Total Data: Seen: 2793, Unseen: 5275\n",
      "\n",
      " ********* Inital ************** \n",
      "\n",
      "Train: Len: 2513, Unique Classes: 6, People: ['1' '10' '4' '5' '8'], Classes Count: Counter({np.int64(1): 428, np.int64(0): 427, np.int64(2): 427, np.int64(3): 427, np.int64(4): 425, np.int64(5): 379})\n",
      "Val: Len: 280, Unique Classes: 6, People: ['1' '10' '4' '5' '8'], Classes Count: Counter({np.int64(0): 48, np.int64(3): 48, np.int64(2): 48, np.int64(1): 47, np.int64(4): 47, np.int64(5): 42})\n",
      "Test: Len: 708, Unique Classes: 6, People: ['1' '10' '4' '5' '8'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(4): 120, np.int64(5): 108})\n",
      "\n",
      " ********* Incremantal ************** \n",
      "\n",
      "Train: Len: 4747, Unique Classes: 12, People: ['2' '3' '6' '7' '9'], Classes Count: Counter({np.int64(2): 428, np.int64(3): 427, np.int64(8): 427, np.int64(10): 427, np.int64(9): 427, np.int64(0): 427, np.int64(1): 427, np.int64(4): 414, np.int64(7): 408, np.int64(5): 403, np.int64(6): 400, np.int64(11): 132}) \n",
      "Val: Len: 528, Unique Classes: 12, People: ['2' '3' '6' '7' '9'], Classes Count: Counter({np.int64(1): 48, np.int64(10): 48, np.int64(9): 48, np.int64(8): 48, np.int64(2): 47, np.int64(0): 47, np.int64(3): 47, np.int64(4): 46, np.int64(7): 45, np.int64(5): 45, np.int64(6): 44, np.int64(11): 15}) \n",
      "Test: Len: 1339 , Unique Classes: 12, People: ['2' '3' '6' '7' '9'], Classes Count: Counter({np.int64(0): 120, np.int64(1): 120, np.int64(2): 120, np.int64(3): 120, np.int64(8): 120, np.int64(9): 120, np.int64(10): 120, np.int64(4): 117, np.int64(7): 115, np.int64(6): 114, np.int64(5): 113, np.int64(11): 40})\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "num_splits = 5\n",
    "\n",
    "# To ensure reproducibility, set a random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "splits = []\n",
    "for _ in range(num_splits):\n",
    "    # Shuffle the array\n",
    "    shuffled_data = np.random.permutation(np.unique(data_p))\n",
    "    # Split the shuffled array into two parts\n",
    "    split_1, split_2 = np.array_split(shuffled_data, 2)\n",
    "    splits.append((split_1, split_2))\n",
    "\n",
    "# Print the results\n",
    "for fold, (train_index, test_index) in enumerate(splits):\n",
    "# for fold, (train_index, test_index) in enumerate(kf.split(np.unique(data_p))):\n",
    "    data_seen_classes_scenario_1_x,data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p = [], [], []\n",
    "    data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p = [], [], []\n",
    "    data_test_scenario_1_x, data_test_scenario_1_y,data_test_scenario_1_p = [], [], []\n",
    "    data_incremental_step_x_test, data_incremental_step_y_test, data_incremental_step_p_test = [], [], []\n",
    "    data_initial_step_x_test, data_initial_step_y_test, data_initial_step_p_test = [], [], []\n",
    "    \n",
    "    data_incremental_step_x_test_new, data_incremental_step_y_test_new, data_incremental_step_p_test_new = [], [], []\n",
    "    data_initial_step_x_test_new, data_initial_step_y_test_new, data_initial_step_p_test_new = [], [], []\n",
    "    \n",
    "    # train_subjects_in_fold = np.unique(data_p)[train_index].tolist()\n",
    "\n",
    "    # Half of the total subjects are in initial step and half are used in incremental step.\n",
    "    # initial_subjects_in_fold = train_subjects_in_fold[0:len(train_subjects_in_fold)//2]\n",
    "    # incremental_subjects_in_fold = train_subjects_in_fold[len(train_subjects_in_fold)//2:]\n",
    "\n",
    "    initial_subjects_in_fold = train_index\n",
    "    incremental_subjects_in_fold = test_index\n",
    "\n",
    "    \n",
    "\n",
    "    # test_subjects_in_fold = np.unique(data_p)[test_index].tolist()\n",
    "\n",
    "    \n",
    "    # print(type(fold), type(list(train_subjects_in_fold.tolist())[0]))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "    print(f\"\\n Fold {fold}, Initial Subjects: {initial_subjects_in_fold}, Incremental Subjects: {incremental_subjects_in_fold} \\n\")\n",
    "\n",
    "    # Getting Last Timestamps for each subject on each activity\n",
    "\n",
    "    # Iterating through each class and each subject.\n",
    "    for inital_class, initial_person in list(itertools.product(initial_step_classes, initial_subjects_in_fold)):\n",
    "        raw_class_data_per_subject = []\n",
    "\n",
    "        # Checking all the datapoints if the class and subject condition match\n",
    "        for index,(person, clas) in enumerate(zip(data_p, data_y)):\n",
    "            if(clas in  [inital_class] and person in [initial_person] ):\n",
    "                raw_class_data_per_subject.append(data_x[index])\n",
    "        \n",
    "        # Adding initial 80% timestampts of the data to training and remaining to test.\n",
    "        train_len_each_person_each_class = int(0.8*len(raw_class_data_per_subject))\n",
    "        test_len_each_person_each_class = len(raw_class_data_per_subject) - int(0.8*len(raw_class_data_per_subject))\n",
    "\n",
    "        data_seen_classes_scenario_1_x.extend(raw_class_data_per_subject[:train_len_each_person_each_class])\n",
    "        data_initial_step_x_test.extend(raw_class_data_per_subject[train_len_each_person_each_class:])\n",
    "        \n",
    "        data_seen_classes_scenario_1_y.extend([inital_class]*train_len_each_person_each_class)\n",
    "        data_initial_step_y_test.extend([inital_class]*test_len_each_person_each_class)\n",
    "\n",
    "        data_seen_classes_scenario_1_p.extend([initial_person]*train_len_each_person_each_class)\n",
    "        data_initial_step_p_test.extend([initial_person]*test_len_each_person_each_class)\n",
    "\n",
    "\n",
    "    # Iterating through each class and each subject.\n",
    "    for incremental_class, incremental_person in list(itertools.product(np.unique(data_y), incremental_subjects_in_fold )):\n",
    "        \n",
    "        raw_class_data_per_subject = []\n",
    "\n",
    "        # Checking all the datapoints if the class and subject condition match\n",
    "        for index,(person, clas) in enumerate(zip(data_p, data_y)):\n",
    "            if(clas in  [incremental_class] and person in [incremental_person] ):\n",
    "                raw_class_data_per_subject.append(data_x[index])\n",
    "        \n",
    "        # Adding initial 80% timestampts of the data to training and remaining to test.\n",
    "        train_len_each_person_each_class = int(0.8*len(raw_class_data_per_subject))\n",
    "        test_len_each_person_each_class = len(raw_class_data_per_subject) - int(0.8*len(raw_class_data_per_subject))\n",
    "\n",
    "        data_unseen_classes_scenario_1_x.extend(raw_class_data_per_subject[:train_len_each_person_each_class])\n",
    "        data_incremental_step_x_test.extend(raw_class_data_per_subject[train_len_each_person_each_class:])\n",
    "        \n",
    "        data_unseen_classes_scenario_1_y.extend([incremental_class]*train_len_each_person_each_class)\n",
    "        data_incremental_step_y_test.extend([incremental_class]*test_len_each_person_each_class)\n",
    "\n",
    "        data_unseen_classes_scenario_1_p.extend([incremental_person]*train_len_each_person_each_class)\n",
    "        data_incremental_step_p_test.extend([incremental_person]*test_len_each_person_each_class)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # for index,person in enumerate(data_p):\n",
    "    #     # print(person, train_subjects_in_fold, data_y[index], initial_step_classes)\n",
    "        \n",
    "     \n",
    "\n",
    "    #     from collections import Counter\n",
    "    #     # For initial step test dataset we only include initial step classes : Old Classes\n",
    "    #     if(person in test_subjects_in_fold and data_y[index] in initial_step_classes):\n",
    "    #         data_initial_step_x_test.append(data_x[index])\n",
    "    #         data_initial_step_y_test.append(data_y[index])\n",
    "    #         data_initial_step_p_test.append(data_p[index])\n",
    "\n",
    "    #     # For Incremental step test dataset we all the classes : Old + New Classes\n",
    "    #     # if(person in test_subjects_in_fold):\n",
    "            \n",
    "    #     #     data_incremental_step_x_test.append(data_x[index])\n",
    "    #     #     data_incremental_step_y_test.append(data_y[index])\n",
    "    #     #     data_incremental_step_p_test.append(data_p[index])\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Doing Split of Initial Step for Validation Data\n",
    "    data_initial_step_scenario_1_x_train, data_initial_step_scenario_1_x_val, data_initial_step_scenario_1_y_train, data_initial_step_scenario_1_y_val, data_initial_step_scenario_1_p_train, data_initial_step_scenario_1_p_val = train_test_split(data_seen_classes_scenario_1_x, data_seen_classes_scenario_1_y, data_seen_classes_scenario_1_p, test_size=0.10, stratify=(data_seen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "\n",
    "    # UnSeen Classes Train and validation Split for Incremental Step. \n",
    "    # Reduce the frequency of seen classes in the incremetnal step\n",
    "    # data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p =  reduce_frequency([data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p], data_unseen_classes_scenario_1_y, initial_step_classes , 1)\n",
    "   \n",
    "    # Getting Validation Data\n",
    "    data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_x_val, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_y_val, data_incremental_step_scenario_1_p_train, data_incremental_step_scenario_1_p_val = train_test_split(data_unseen_classes_scenario_1_x, data_unseen_classes_scenario_1_y, data_unseen_classes_scenario_1_p, test_size=0.1, stratify=(data_unseen_classes_scenario_1_y), random_state=42)\n",
    "\n",
    "    # Removing some of the seen data from the training step.\n",
    "    # data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_p_train = reduce_frequency([data_incremental_step_scenario_1_x_train, data_incremental_step_scenario_1_y_train, data_incremental_step_scenario_1_p_train], data_incremental_step_scenario_1_y_train, initial_step_classes , 0.0)\n",
    "    \n",
    "    print(f\"Total Data: Seen: {len(data_seen_classes_scenario_1_x)}, Unseen: {len(data_unseen_classes_scenario_1_x)}\")\n",
    "    print(\"\\n ********* Inital ************** \\n\")\n",
    "    print(f\"Train: Len: {len(data_initial_step_scenario_1_x_train)}, Unique Classes: {len(np.unique(data_initial_step_scenario_1_y_train))}, People: {np.unique(data_initial_step_scenario_1_p_train)}, Classes Count: {Counter(data_initial_step_scenario_1_y_train)}\")\n",
    "    print(f\"Val: Len: {len(data_initial_step_scenario_1_x_val)}, Unique Classes: {len(np.unique(data_initial_step_scenario_1_y_val))}, People: {np.unique(data_initial_step_scenario_1_p_val)}, Classes Count: {Counter(data_initial_step_scenario_1_y_val)}\") \n",
    "    print(f\"Test: Len: {len(data_initial_step_x_test)}, Unique Classes: {len(np.unique(data_initial_step_y_test))}, People: {np.unique(data_initial_step_p_test)}, Classes Count: {Counter(data_initial_step_y_test)}\")\n",
    "    # print(f\"Test New: Len: {len(data_initial_step_x_test_new)}, Unique Classes: {len(np.unique(data_initial_step_y_test_new))}, People: {np.unique(data_initial_step_p_test_new)}, Classes Count: {Counter(data_initial_step_y_test_new)}\")\n",
    "\n",
    "    print(\"\\n ********* Incremantal ************** \\n\")\n",
    "    print(f\"Train: Len: {len(data_incremental_step_scenario_1_x_train)}, Unique Classes: {len(np.unique(data_incremental_step_scenario_1_y_train))}, People: {np.unique(data_incremental_step_scenario_1_p_train)}, Classes Count: {Counter(data_incremental_step_scenario_1_y_train)} \")\n",
    "    print(f\"Val: Len: {len(data_incremental_step_scenario_1_x_val)}, Unique Classes: {len(np.unique(data_incremental_step_scenario_1_y_val))}, People: {np.unique(data_incremental_step_scenario_1_p_val)}, Classes Count: {Counter(data_incremental_step_scenario_1_y_val)} \")    \n",
    "    print(f\"Test: Len: {len(data_incremental_step_x_test)} , Unique Classes: {len(np.unique(data_incremental_step_y_test))}, People: {np.unique(data_incremental_step_p_test)}, Classes Count: {Counter(data_incremental_step_y_test)}\")\n",
    "    # print(f\"Test New: Len: {len(data_incremental_step_x_test_new)} , Unique Classes: {len(np.unique(data_incremental_step_y_test_new))}, People: {np.unique(data_incremental_step_p_test_new)}, Classes Count: {Counter(data_incremental_step_y_test_new)}\")\n",
    "    \n",
    "\n",
    "    # Saving the necessary files\n",
    "    windowlen = np.array(data_incremental_step_scenario_1_x_train).shape[1]\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_scenario_1_p_train)\n",
    "\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_initial_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_initial_step_p_test)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_scenario_1_x_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_scenario_1_y_train)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_train_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_scenario_1_p_train)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_scenario_1_x_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_scenario_1_y_val)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_val_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_scenario_1_p_val)\n",
    "   \n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_x_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_x_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_y_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_y_test)\n",
    "    np.save(f'HAR_data/{dataset}/data_incremental_step_scenario_1_p_test_windowLen_{str(windowlen)}_standardized_{standardize}_fold{fold}_session.npy',data_incremental_step_p_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
